{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":223116017,"sourceType":"kernelVersion"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/liuserr/notebook2313628fdd?scriptVersionId=223484009\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Task B: FSDP2 with QLoRA\nThis notebook demonstrates a functional application of QLoRA using FSDP2. The acceleration is moderate, due to large overhead. Here are the three loss curves generated. 1 is using FSDP1 on a single GPU. 2 is using FSDP2 without gradient accumulation, making it more susceptible to memory overload. 3 is using FSDP2 with gradient accumulation. As you can see, the loss curves are the same for all configurations.\n\n*FSDP1*\n\n![FSDP1](loss_plot.png \"FSDP1\") \n\n*FSDP2 No Gradient Accum*\n\n![alt text](loss_plot_v2.png \"FSDP2 No Gradient Accum\") \n\n*FSDP2 Gradient Accum*\n\n![alt text](loss_plot_gradient_accum.png \"FSDP2 Gradient Accum\")","metadata":{}},{"cell_type":"code","source":"!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n!pip install --no-deps cut_cross_entropy unsloth_zoo\n!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n!pip install --no-deps unsloth","metadata":{"_uuid":"8a016f92-dc94-4b90-af9e-2a7403e781ea","_cell_guid":"f26ad1bf-0b18-4963-8709-3b49c8587b66","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U torch accelerate transformers bitsandbytes peft","metadata":{"_uuid":"f90269c9-ffcf-4fc2-b7ec-c1220d79aaae","_cell_guid":"4d59d6b2-9da4-4a1b-8e80-0762fc233278","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade torchvision torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall \"numpy<2.0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install importlib-metadata","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Single GPU FSDP1\n\n![image](loss_plot.png)","metadata":{}},{"cell_type":"code","source":"%%writefile single_gpu.py\nimport os\nimport random\nimport numpy as np\nimport torch\n\nseed = 3407\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# --- Environment Setup ---\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    CPUOffload,\n    ShardingStrategy,\n    MixedPrecision,\n)\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\nfrom functools import partial\n\n# --- 1) Distributed Setup ---\nif \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n    dist.init_process_group(backend=\"nccl\")\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\nelse:\n    local_rank = 0\n\naccelerator = Accelerator(mixed_precision=\"bf16\")\ndevice = torch.device(\"cuda\", local_rank)\n\n# --- 2) Model & 4-bit Quantization ---\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\ndtype = torch.bfloat16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n    bnb_4bit_quant_storage=dtype,  # try storing quantized weights as BF16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 3) Apply LoRA ---\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\nfor name, param in model.named_parameters():\n    if \".lora_\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# --- Integrate torch.compile (PyTorch 2.0 feature) ---\ntry:\n    model = torch.compile(model)\n    accelerator.print(\"torch.compile: Model compiled successfully.\")\nexcept Exception as e:\n    accelerator.print(f\"torch.compile: Compilation failed with error: {e}. Continuing without compilation.\")\n\ntorch.cuda.empty_cache()\n\n# --- Debug: Print buffer names and dtypes ---\nif local_rank == 0:\n    buffer_info = [(name, buf.dtype) for name, buf in model.named_buffers()]\n    print(\"Buffers in model (name, dtype):\")\n    for name, dt in buffer_info:\n        print(name, dt)\n\n# --- 5) FSDP Wrapping with FSDP1 ---\n# Ignore the frozen base module from FSDP sharding.\nbase_model_submodule = model.model  \nignored_modules = [base_model_submodule]\n\nauto_wrap_policy = partial(\n    size_based_auto_wrap_policy,\n    min_num_params=5e7,\n    recurse=True,\n    nonwrapped_numel=0,\n)\n\nmodel = FSDP(\n    model,\n    auto_wrap_policy=auto_wrap_policy,\n    sharding_strategy=ShardingStrategy.FULL_SHARD,  # Ensure full sharding (FSDP1)\n    use_orig_params=True,\n    mixed_precision=MixedPrecision(param_dtype=dtype),\n    ignored_modules=ignored_modules,\n    device_id=device,\n)\n\n# --- 6) Tokenizer & Dataset ---\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n\ndataset = load_dataset(\n    \"json\",\n    data_files={\"train\": \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"},\n    split=\"train[:10%]\"\n)\n\ndef tokenize_example(ex):\n    enc = tokenizer(\n        ex[\"text\"],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n    return enc\n\ndataset = dataset.map(tokenize_example, batched=True, remove_columns=[\"text\"])\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntrain_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\ntrain_dataloader = accelerator.prepare(train_dataloader)\n\n# --- 7) Optimizer ---\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\ntry:\n    from bitsandbytes.optim import Adam8bit\n    optimizer = Adam8bit(trainable_params, lr=2e-4)\n    accelerator.print(\"Using bitsandbytes Adam8bit optimizer.\")\nexcept ImportError:\n    optimizer = torch.optim.AdamW(trainable_params, lr=2e-4)\n    accelerator.print(\"Using torch.optim.AdamW optimizer.\")\noptimizer = accelerator.prepare(optimizer)\n\n# --- 8) Training Loop with Gradient Accumulation ---\nloss_history = []\nmodel.train()\ngradient_accumulation_steps = 16  # Adjust this value as needed\n\nfor step, batch in enumerate(train_dataloader, start=1):\n    # The accelerator.accumulate context will accumulate gradients over the specified steps.\n    with accelerator.accumulate(model):\n        outputs = model(**batch)\n        # Scale loss to account for gradient accumulation\n        loss = outputs.loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n        # Clear cache after the optimizer step to lower VRAM usage\n        if accelerator.sync_gradients:\n            torch.cuda.empty_cache()\n    \n    if accelerator.is_main_process:\n        loss_history.append((step, outputs.loss.item()))\n        accelerator.print(f\"Step {step} - Loss: {outputs.loss.item():.4f}\")\n    \n    # Terminate after 60 steps (micro-batches)\n    if step >= 60:\n        break\naccelerator.wait_for_everyone()\n\nif accelerator.is_main_process:\n    try:\n        import matplotlib.pyplot as plt\n        steps, losses = zip(*loss_history) if loss_history else ([], [])\n        if steps:\n            plt.figure(figsize=(8, 4))\n            plt.plot(steps, losses, marker=\"o\")\n            plt.xlabel(\"Training Step\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss over the First 60 Training Steps\")\n            plt.grid(True)\n            plt.savefig(\"loss_plot.png\")\n            plt.show()\n        else:\n            print(\"No loss data recorded.\")\n    except ImportError:\n        accelerator.print(\"matplotlib is not installed. Skipping loss plot.\")\n\n# --- 9) Save Model ---\nif accelerator.is_main_process:\n    final_model = accelerator.unwrap_model(model)\n    final_model.save_pretrained(\"llama-8b-finetuned\", safe_serialization=True)\n    tokenizer.save_pretrained(\"llama-8b-finetuned\")\n\nif dist.is_initialized():\n    dist.destroy_process_group()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T22:05:34.691897Z","iopub.execute_input":"2025-02-19T22:05:34.692233Z","iopub.status.idle":"2025-02-19T22:05:34.698182Z","shell.execute_reply.started":"2025-02-19T22:05:34.692179Z","shell.execute_reply":"2025-02-19T22:05:34.69737Z"}},"outputs":[{"name":"stdout","text":"Overwriting single_gpu.py\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!torchrun --nproc_per_node=2 --nnodes=1 single_gpu.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T22:05:45.946481Z","iopub.execute_input":"2025-02-19T22:05:45.946766Z","iopub.status.idle":"2025-02-19T22:15:35.448851Z","shell.execute_reply.started":"2025-02-19T22:05:45.946744Z","shell.execute_reply":"2025-02-19T22:15:35.447504Z"}},"outputs":[{"name":"stdout","text":"W0219 22:05:47.889000 134 torch/distributed/run.py:792] \nW0219 22:05:47.889000 134 torch/distributed/run.py:792] *****************************************\nW0219 22:05:47.889000 134 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0219 22:05:47.889000 134 torch/distributed/run.py:792] *****************************************\n2025-02-19 22:05:56.279975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-19 22:05:56.279957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-19 22:05:56.469977: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-19 22:05:56.469991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-19 22:05:56.524450: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-19 22:05:56.524460: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nconfig.json: 100%|█████████████████████████| 1.53k/1.53k [00:00<00:00, 9.83MB/s]\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nmodel.safetensors: 100%|███████████████████▉| 5.70G/5.70G [00:11<00:00, 495MB/s]\ngeneration_config.json: 100%|██████████████████| 239/239 [00:00<00:00, 1.34MB/s]\ntokenizer_config.json: 100%|███████████████| 55.5k/55.5k [00:00<00:00, 5.60MB/s]\ntorch.compile: Model compiled successfully.\ntokenizer.json:   0%|                               | 0.00/17.2M [00:00<?, ?B/s]Buffers in model (name, dtype):\n_orig_mod.base_model.model.model.rotary_emb.inv_freq torch.float32\ntokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:00<00:00, 39.7MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 454/454 [00:00<00:00, 2.22MB/s]\nunified_chip2.jsonl: 100%|█████████████████▉| 95.6M/95.6M [00:00<00:00, 185MB/s]\nGenerating train split: 210289 examples [00:00, 275388.00 examples/s]\nMap: 100%|███████████████████████| 21029/21029 [00:11<00:00, 1870.28 examples/s]\nUsing bitsandbytes Adam8bit optimizer.\nMap: 100%|███████████████████████| 21029/21029 [00:11<00:00, 1835.42 examples/s]\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n[rank1]:W0219 22:06:55.405000 140 torch/_inductor/utils.py:1137] [12/0] Not enough SMs to use max_autotune_gemm mode\n[rank0]:W0219 22:06:55.818000 139 torch/_inductor/utils.py:1137] [12/0] Not enough SMs to use max_autotune_gemm mode\nStep 1 - Loss: 13.4110\nStep 2 - Loss: 10.5894\nStep 3 - Loss: 7.4178\nStep 4 - Loss: 7.0390\nStep 5 - Loss: 6.2871\nStep 6 - Loss: 6.2040\nStep 7 - Loss: 5.8364\nStep 8 - Loss: 6.0813\nStep 9 - Loss: 5.9933\nStep 10 - Loss: 6.0208\nStep 11 - Loss: 5.2397\nStep 12 - Loss: 4.8746\nStep 13 - Loss: 5.4578\nStep 14 - Loss: 5.7544\nStep 15 - Loss: 5.5417\nStep 16 - Loss: 5.6638\nStep 17 - Loss: 5.4860\nStep 18 - Loss: 5.5214\nStep 19 - Loss: 5.3237\nStep 20 - Loss: 5.4889\nStep 21 - Loss: 5.3215\nStep 22 - Loss: 5.3806\nStep 23 - Loss: 5.1313\nStep 24 - Loss: 5.3849\nStep 25 - Loss: 4.9107\nStep 26 - Loss: 4.7860\nStep 27 - Loss: 5.6548\nStep 28 - Loss: 5.1008\nStep 29 - Loss: 5.1236\nStep 30 - Loss: 5.2470\nStep 31 - Loss: 5.3374\nStep 32 - Loss: 5.0211\nStep 33 - Loss: 5.4820\nStep 34 - Loss: 5.2197\nStep 35 - Loss: 3.7071\nStep 36 - Loss: 5.4851\nStep 37 - Loss: 5.0779\nStep 38 - Loss: 5.2887\nStep 39 - Loss: 5.5702\nStep 40 - Loss: 4.5186\nStep 41 - Loss: 5.0148\nStep 42 - Loss: 5.1218\nStep 43 - Loss: 5.5554\nStep 44 - Loss: 5.2955\nStep 45 - Loss: 5.5776\nStep 46 - Loss: 5.2851\nStep 47 - Loss: 5.0836\nStep 48 - Loss: 3.6111\nStep 49 - Loss: 5.4449\nStep 50 - Loss: 5.3159\nStep 51 - Loss: 5.6011\nStep 52 - Loss: 5.5691\nStep 53 - Loss: 4.3145\nStep 54 - Loss: 4.1124\nStep 55 - Loss: 5.1970\nStep 56 - Loss: 5.4249\nStep 57 - Loss: 5.2037\nStep 58 - Loss: 5.5513\nStep 59 - Loss: 4.9018\nStep 60 - Loss: 5.2243\nFigure(800x400)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# FSDP2 without Gradient Accumulation\n![image](loss_plot_v2.png)","metadata":{}},{"cell_type":"code","source":"%%writefile finetune_llama31_8b_fsdp2_qlora_revised_v2.py\nimport os\n\nimport random\nimport numpy as np\nimport torch\n\nseed = 3407\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Optionally, enforce determinism (may slow down training):\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False\n\n\n# --- Environment Setup ---\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    CPUOffload,\n    ShardingStrategy,\n    MixedPrecision,\n)\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\nfrom functools import partial\n\n# --- 1) Distributed Setup ---\nif \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n    dist.init_process_group(backend=\"nccl\")\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\nelse:\n    local_rank = 0\n\naccelerator = Accelerator(mixed_precision=\"bf16\")\ndevice = torch.device(\"cuda\", local_rank)\n\n# --- 2) Model & 4-bit Quantization ---\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\ndtype = torch.bfloat16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n    bnb_4bit_quant_storage=dtype,  # try storing quantized weights as BF16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 3) Apply LoRA ---\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\nfor name, param in model.named_parameters():\n    if \".lora_\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# --- Integrate torch.compile (PyTorch 2.0 feature) ---\ntry:\n    model = torch.compile(model)\n    accelerator.print(\"torch.compile: Model compiled successfully.\")\nexcept Exception as e:\n    accelerator.print(f\"torch.compile: Compilation failed with error: {e}. Continuing without compilation.\")\n\n# --- Optionally, we previously converted int parameters to buffers.\n# For this version, we skip that conversion and rely on bitsandbytes to manage quantized weights.\n# def convert_int_params_to_buffers(module: torch.nn.Module):\n#     for name, param in list(module.named_parameters(recurse=False)):\n#         if param is not None and not param.dtype.is_floating_point:\n#             del module._parameters[name]\n#             module.register_buffer(name, param.data)\n#     for child in module.children():\n#         convert_int_params_to_buffers(child)\n#\n# convert_int_params_to_buffers(model)\n\ntorch.cuda.empty_cache()\n\n# --- Debug: Print buffer names and dtypes ---\nif local_rank == 0:\n    buffer_info = [(name, buf.dtype) for name, buf in model.named_buffers()]\n    print(\"Buffers in model (name, dtype):\")\n    for name, dt in buffer_info:\n        print(name, dt)\n\n# --- 5) FSDP Wrapping ---\n# Ignore the frozen base module from FSDP sharding.\nbase_model_submodule = model.model  \nignored_modules = [base_model_submodule]\n\nauto_wrap_policy = partial(\n    size_based_auto_wrap_policy,\n    min_num_params=5e7,\n    recurse=True,\n    nonwrapped_numel=0,\n)\n\nmp_policy = MixedPrecision(\n    param_dtype=dtype,      # cast parameters to BF16\n    reduce_dtype=dtype,     # use BF16 for gradient reduction\n    buffer_dtype=None,      # do not cast buffers\n)\n\ncpu_offload = None\n# Uncomment the next line if needed:\n# cpu_offload = CPUOffload(offload_params=True)\n\nmodel = FSDP(\n    model,\n    auto_wrap_policy=auto_wrap_policy,\n    sharding_strategy=ShardingStrategy.FULL_SHARD,\n    use_orig_params=True,\n    cpu_offload=cpu_offload,\n    limit_all_gathers=True,\n    device_id=device,\n    ignored_modules=ignored_modules,\n    mixed_precision=mp_policy,\n)\n\n# --- 6) Tokenizer & Dataset ---\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n\ndataset = load_dataset(\n    \"json\",\n    data_files={\"train\": \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"},\n    split=\"train[:10%]\"\n)\n\ndef tokenize_example(ex):\n    enc = tokenizer(\n        ex[\"text\"],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n    return enc\n\ndataset = dataset.map(tokenize_example, batched=True, remove_columns=[\"text\"])\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntrain_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\ntrain_dataloader = accelerator.prepare(train_dataloader)\n\n# --- 7) Optimizer ---\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\ntry:\n    from bitsandbytes.optim import Adam8bit\n    optimizer = Adam8bit(trainable_params, lr=2e-4)\n    accelerator.print(\"Using bitsandbytes Adam8bit optimizer.\")\nexcept ImportError:\n    optimizer = torch.optim.AdamW(trainable_params, lr=2e-4)\n    accelerator.print(\"Using torch.optim.AdamW optimizer.\")\noptimizer = accelerator.prepare(optimizer)\n\n# --- 8) Training Loop ---\nloss_history = []\nmodel.train()\nfor step, batch in enumerate(train_dataloader, start=1):\n    outputs = model(**batch)\n    loss = outputs.loss\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n    torch.cuda.empty_cache()\n    if accelerator.is_main_process:\n        loss_history.append((step, loss.item()))\n    \n    accelerator.print(f\"Step {step} - Loss: {loss.item():.4f}\")\n    \n    # Terminate after 60 steps.\n    if step >= 60:\n        break\naccelerator.wait_for_everyone()\n\nif accelerator.is_main_process:\n    try:\n        import matplotlib.pyplot as plt\n        steps, losses = zip(*loss_history) if loss_history else ([], [])\n        if steps:\n            plt.figure(figsize=(8, 4))\n            plt.plot(steps, losses, marker=\"o\")\n            plt.xlabel(\"Training Step\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss over the First 60 Training Steps\")\n            plt.grid(True)\n            plt.savefig(\"loss_plot_v2.png\")\n            plt.show()\n        else:\n            print(\"No loss data recorded.\")\n    except ImportError:\n        accelerator.print(\"matplotlib is not installed. Skipping loss plot.\")\n\n\n# --- 9) Save Model ---\nif accelerator.is_main_process:\n    final_model = accelerator.unwrap_model(model)\n    final_model.save_pretrained(\"llama-8b-finetuned\", safe_serialization=True)\n    tokenizer.save_pretrained(\"llama-8b-finetuned\")\n\nif dist.is_initialized():\n    dist.destroy_process_group()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T22:16:18.502238Z","iopub.execute_input":"2025-02-19T22:16:18.502602Z","iopub.status.idle":"2025-02-19T22:16:18.509722Z","shell.execute_reply.started":"2025-02-19T22:16:18.502573Z","shell.execute_reply":"2025-02-19T22:16:18.508739Z"}},"outputs":[{"name":"stdout","text":"Writing finetune_llama31_8b_fsdp2_qlora_revised_v2.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!accelerate launch --num_processes=2 --mixed_precision=bf16 finetune_llama31_8b_fsdp2_qlora_revised_v2.py","metadata":{"_uuid":"ca3d9d5c-55c6-40ee-a924-0339e2cf6a81","_cell_guid":"d9fc9790-f801-4b76-af39-38bd4a1c11ba","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-02-19T22:16:21.967255Z","iopub.execute_input":"2025-02-19T22:16:21.967587Z","iopub.status.idle":"2025-02-19T22:25:39.347722Z","shell.execute_reply.started":"2025-02-19T22:16:21.967561Z","shell.execute_reply":"2025-02-19T22:25:39.346883Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"2025-02-19 22:16:34.708720: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-19 22:16:34.708846: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-19 22:16:34.731531: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-19 22:16:34.731603: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-19 22:16:34.738261: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-19 22:16:34.738262: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\ntorch.compile: Model compiled successfully.\nBuffers in model (name, dtype):\n_orig_mod.base_model.model.model.rotary_emb.inv_freq torch.float32\nMap: 100%|███████████████████████| 21029/21029 [00:11<00:00, 1874.14 examples/s]\nMap: 100%|███████████████████████| 21029/21029 [00:11<00:00, 1865.76 examples/s]\nUsing bitsandbytes Adam8bit optimizer.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nStep 1 - Loss: 13.4110\nStep 2 - Loss: 10.5894\nStep 3 - Loss: 7.4176\nStep 4 - Loss: 7.0388\nStep 5 - Loss: 6.2866\nStep 6 - Loss: 6.2036\nStep 7 - Loss: 5.8367\nStep 8 - Loss: 6.0814\nStep 9 - Loss: 5.9930\nStep 10 - Loss: 6.0204\nStep 11 - Loss: 5.2393\nStep 12 - Loss: 4.8745\nStep 13 - Loss: 5.4576\nStep 14 - Loss: 5.7544\nStep 15 - Loss: 5.5415\nStep 16 - Loss: 5.6639\nStep 17 - Loss: 5.4858\nStep 18 - Loss: 5.5213\nStep 19 - Loss: 5.3234\nStep 20 - Loss: 5.4890\nStep 21 - Loss: 5.3212\nStep 22 - Loss: 5.3807\nStep 23 - Loss: 5.1311\nStep 24 - Loss: 5.3850\nStep 25 - Loss: 4.9108\nStep 26 - Loss: 4.7860\nStep 27 - Loss: 5.6550\nStep 28 - Loss: 5.1008\nStep 29 - Loss: 5.1234\nStep 30 - Loss: 5.2471\nStep 31 - Loss: 5.3379\nStep 32 - Loss: 5.0213\nStep 33 - Loss: 5.4812\nStep 34 - Loss: 5.2199\nStep 35 - Loss: 3.7060\nStep 36 - Loss: 5.3955\nStep 37 - Loss: 5.0802\nStep 38 - Loss: 5.2874\nStep 39 - Loss: 5.5727\nStep 40 - Loss: 4.5248\nStep 41 - Loss: 5.0163\nStep 42 - Loss: 5.1260\nStep 43 - Loss: 5.5578\nStep 44 - Loss: 5.2935\nStep 45 - Loss: 5.5716\nStep 46 - Loss: 5.2882\nStep 47 - Loss: 5.0867\nStep 48 - Loss: 3.5965\nStep 49 - Loss: 5.4513\nStep 50 - Loss: 5.3090\nStep 51 - Loss: 5.5996\nStep 52 - Loss: 5.5674\nStep 53 - Loss: 4.2551\nStep 54 - Loss: 4.0876\nStep 55 - Loss: 5.2041\nStep 56 - Loss: 5.4313\nStep 57 - Loss: 5.2149\nStep 58 - Loss: 5.5501\nStep 59 - Loss: 4.9108\nStep 60 - Loss: 5.2265\nFigure(800x400)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# FSDP2 with Gradient Accumulation\n![image](loss_plot_gradient_accum.png)","metadata":{}},{"cell_type":"code","source":"%%writefile gradient_accumulation.py\nimport os\n\nimport random\nimport numpy as np\nimport torch\n\nseed = 3407\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Optionally, enforce determinism (may slow down training):\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False\n\n\n# --- Environment Setup ---\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    CPUOffload,\n    ShardingStrategy,\n    MixedPrecision,\n)\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\nfrom functools import partial\n\n# --- 1) Distributed Setup ---\nif \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n    dist.init_process_group(backend=\"nccl\")\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\nelse:\n    local_rank = 0\n\naccelerator = Accelerator(mixed_precision=\"bf16\")\ndevice = torch.device(\"cuda\", local_rank)\n\n# --- 2) Model & 4-bit Quantization ---\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\ndtype = torch.bfloat16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n    bnb_4bit_quant_storage=dtype,  # try storing quantized weights as BF16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 3) Apply LoRA ---\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\nfor name, param in model.named_parameters():\n    if \".lora_\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# --- Integrate torch.compile (PyTorch 2.0 feature) ---\ntry:\n    model = torch.compile(model)\n    accelerator.print(\"torch.compile: Model compiled successfully.\")\nexcept Exception as e:\n    accelerator.print(f\"torch.compile: Compilation failed with error: {e}. Continuing without compilation.\")\n\n# --- Optionally, we previously converted int parameters to buffers.\n# For this version, we skip that conversion and rely on bitsandbytes to manage quantized weights.\n# def convert_int_params_to_buffers(module: torch.nn.Module):\n#     for name, param in list(module.named_parameters(recurse=False)):\n#         if param is not None and not param.dtype.is_floating_point:\n#             del module._parameters[name]\n#             module.register_buffer(name, param.data)\n#     for child in module.children():\n#         convert_int_params_to_buffers(child)\n#\n# convert_int_params_to_buffers(model)\n\ntorch.cuda.empty_cache()\n\n# --- Debug: Print buffer names and dtypes ---\nif local_rank == 0:\n    buffer_info = [(name, buf.dtype) for name, buf in model.named_buffers()]\n    print(\"Buffers in model (name, dtype):\")\n    for name, dt in buffer_info:\n        print(name, dt)\n\n# --- 5) FSDP Wrapping ---\n# Ignore the frozen base module from FSDP sharding.\nbase_model_submodule = model.model  \nignored_modules = [base_model_submodule]\n\nauto_wrap_policy = partial(\n    size_based_auto_wrap_policy,\n    min_num_params=5e7,\n    recurse=True,\n    nonwrapped_numel=0,\n)\n\nmp_policy = MixedPrecision(\n    param_dtype=dtype,      # cast parameters to BF16\n    reduce_dtype=dtype,     # use BF16 for gradient reduction\n    buffer_dtype=None,      # do not cast buffers\n)\n\ncpu_offload = None\n# Uncomment the next line if needed:\n# cpu_offload = CPUOffload(offload_params=True)\n\nmodel = FSDP(\n    model,\n    auto_wrap_policy=auto_wrap_policy,\n    sharding_strategy=ShardingStrategy.FULL_SHARD,\n    use_orig_params=True,\n    cpu_offload=cpu_offload,\n    limit_all_gathers=True,\n    device_id=device,\n    ignored_modules=ignored_modules,\n    mixed_precision=mp_policy,\n)\n\n# --- 6) Tokenizer & Dataset ---\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n\ndataset = load_dataset(\n    \"json\",\n    data_files={\"train\": \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"},\n    split=\"train[:10%]\"\n)\n\ndef tokenize_example(ex):\n    enc = tokenizer(\n        ex[\"text\"],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n    return enc\n\ndataset = dataset.map(tokenize_example, batched=True, remove_columns=[\"text\"])\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntrain_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\ntrain_dataloader = accelerator.prepare(train_dataloader)\n\n# --- 7) Optimizer ---\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\ntry:\n    from bitsandbytes.optim import Adam8bit\n    optimizer = Adam8bit(trainable_params, lr=2e-4)\n    accelerator.print(\"Using bitsandbytes Adam8bit optimizer.\")\nexcept ImportError:\n    optimizer = torch.optim.AdamW(trainable_params, lr=2e-4)\n    accelerator.print(\"Using torch.optim.AdamW optimizer.\")\noptimizer = accelerator.prepare(optimizer)\n\n# --- 8) Training Loop with Gradient Accumulation ---\nloss_history = []\nmodel.train()\ngradient_accumulation_steps = 16  # Adjust this value as needed\n\nfor step, batch in enumerate(train_dataloader, start=1):\n    # The accelerator.accumulate context will accumulate gradients over the specified steps.\n    with accelerator.accumulate(model):\n        outputs = model(**batch)\n        # Scale loss to account for gradient accumulation\n        loss = outputs.loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n        # Clear cache after the optimizer step to lower VRAM usage\n        if accelerator.sync_gradients:\n            torch.cuda.empty_cache()\n    \n    if accelerator.is_main_process:\n        loss_history.append((step, outputs.loss.item()))\n        accelerator.print(f\"Step {step} - Loss: {outputs.loss.item():.4f}\")\n    \n    # Terminate after 60 steps (micro-batches)\n    if step >= 60:\n        break\naccelerator.wait_for_everyone()\n\nif accelerator.is_main_process:\n    try:\n        import matplotlib.pyplot as plt\n        steps, losses = zip(*loss_history) if loss_history else ([], [])\n        if steps:\n            plt.figure(figsize=(8, 4))\n            plt.plot(steps, losses, marker=\"o\")\n            plt.xlabel(\"Training Step\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss over the First 60 Training Steps\")\n            plt.grid(True)\n            plt.savefig(\"loss_plot_gradient_accum.png\")\n            plt.show()\n        else:\n            print(\"No loss data recorded.\")\n    except ImportError:\n        accelerator.print(\"matplotlib is not installed. Skipping loss plot.\")\n\n\n# --- 9) Save Model ---\nif accelerator.is_main_process:\n    final_model = accelerator.unwrap_model(model)\n    final_model.save_pretrained(\"llama-8b-finetuned\", safe_serialization=True)\n    tokenizer.save_pretrained(\"llama-8b-finetuned\")\n\nif dist.is_initialized():\n    dist.destroy_process_group()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T22:31:34.272751Z","iopub.execute_input":"2025-02-19T22:31:34.273147Z","iopub.status.idle":"2025-02-19T22:31:34.280282Z","shell.execute_reply.started":"2025-02-19T22:31:34.273109Z","shell.execute_reply":"2025-02-19T22:31:34.279421Z"}},"outputs":[{"name":"stdout","text":"Writing gradient_accumulation.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!accelerate launch --num_processes=2 --mixed_precision=bf16 gradient_accumulation.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T22:31:39.517052Z","iopub.execute_input":"2025-02-19T22:31:39.517379Z","iopub.status.idle":"2025-02-19T22:40:51.59629Z","shell.execute_reply.started":"2025-02-19T22:31:39.517354Z","shell.execute_reply":"2025-02-19T22:40:51.595087Z"}},"outputs":[{"name":"stdout","text":"2025-02-19 22:31:49.962854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-19 22:31:49.975927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-02-19 22:31:49.985704: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-19 22:31:49.992782: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-02-19 22:31:49.997723: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-02-19 22:31:50.004384: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:206: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\ntorch.compile: Model compiled successfully.\nBuffers in model (name, dtype):\n_orig_mod.base_model.model.model.rotary_emb.inv_freq torch.float32\nMap: 100%|███████████████████████| 21029/21029 [00:11<00:00, 1758.05 examples/s]\nMap: 100%|███████████████████████| 21029/21029 [00:11<00:00, 1784.22 examples/s]\nUsing bitsandbytes Adam8bit optimizer.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\nStep 1 - Loss: 13.4110\nStep 2 - Loss: 10.5894\nStep 3 - Loss: 7.4178\nStep 4 - Loss: 7.0390\nStep 5 - Loss: 6.2871\nStep 6 - Loss: 6.2040\nStep 7 - Loss: 5.8364\nStep 8 - Loss: 6.0813\nStep 9 - Loss: 5.9933\nStep 10 - Loss: 6.0208\nStep 11 - Loss: 5.2397\nStep 12 - Loss: 4.8746\nStep 13 - Loss: 5.4578\nStep 14 - Loss: 5.7544\nStep 15 - Loss: 5.5417\nStep 16 - Loss: 5.6638\nStep 17 - Loss: 5.4860\nStep 18 - Loss: 5.5214\nStep 19 - Loss: 5.3237\nStep 20 - Loss: 5.4889\nStep 21 - Loss: 5.3215\nStep 22 - Loss: 5.3806\nStep 23 - Loss: 5.1313\nStep 24 - Loss: 5.3849\nStep 25 - Loss: 4.9107\nStep 26 - Loss: 4.7860\nStep 27 - Loss: 5.6548\nStep 28 - Loss: 5.1008\nStep 29 - Loss: 5.1236\nStep 30 - Loss: 5.2470\nStep 31 - Loss: 5.3374\nStep 32 - Loss: 5.0211\nStep 33 - Loss: 5.4820\nStep 34 - Loss: 5.2197\nStep 35 - Loss: 3.7071\nStep 36 - Loss: 5.4851\nStep 37 - Loss: 5.0779\nStep 38 - Loss: 5.2887\nStep 39 - Loss: 5.5702\nStep 40 - Loss: 4.5186\nStep 41 - Loss: 5.0148\nStep 42 - Loss: 5.1218\nStep 43 - Loss: 5.5554\nStep 44 - Loss: 5.2955\nStep 45 - Loss: 5.5776\nStep 46 - Loss: 5.2851\nStep 47 - Loss: 5.0836\nStep 48 - Loss: 3.6111\nStep 49 - Loss: 5.4449\nStep 50 - Loss: 5.3159\nStep 51 - Loss: 5.6011\nStep 52 - Loss: 5.5691\nStep 53 - Loss: 4.3145\nStep 54 - Loss: 4.1124\nStep 55 - Loss: 5.1970\nStep 56 - Loss: 5.4249\nStep 57 - Loss: 5.2037\nStep 58 - Loss: 5.5513\nStep 59 - Loss: 4.9018\nStep 60 - Loss: 5.2243\nFigure(800x400)\n","output_type":"stream"}],"execution_count":13}]}