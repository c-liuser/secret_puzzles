{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":223116017,"sourceType":"kernelVersion"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/liuserr/notebook2313628fdd?scriptVersionId=225601960\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Apache 2.0 License\n","metadata":{"_kg_hide-output":true}},{"cell_type":"markdown","source":"Copyright (C) 2025 Cole Liu\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n","metadata":{}},{"cell_type":"markdown","source":"# Task B: FSDP2 with QLoRA\nThis notebook demonstrates a functional application of QLoRA using FSDP2. The acceleration is moderate, due to large overhead. Here are the three loss curves generated. 1 is using FSDP1 on a single GPU. 2 is using FSDP2 without gradient accumulation, making it more susceptible to memory overload. 3 is using FSDP2 with gradient accumulation. As you can see, the loss curves are the same for all configurations.\n\n*FSDP1*\n\n![FSDP1](loss_plot.png \"FSDP1\") \n\n*FSDP2 No Gradient Accum*\n\n![alt text](loss_plot_v2.png \"FSDP2 No Gradient Accum\") \n\n*FSDP2 Gradient Accum*\n\n![alt text](loss_plot_gradient_accum.png \"FSDP2 Gradient Accum\")","metadata":{}},{"cell_type":"code","source":"!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n!pip install --no-deps cut_cross_entropy unsloth_zoo\n!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n!pip install --no-deps unsloth","metadata":{"_uuid":"8a016f92-dc94-4b90-af9e-2a7403e781ea","_cell_guid":"f26ad1bf-0b18-4963-8709-3b49c8587b66","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-04T00:55:08.267272Z","iopub.execute_input":"2025-03-04T00:55:08.267583Z","iopub.status.idle":"2025-03-04T00:55:29.888643Z","shell.execute_reply.started":"2025-03-04T00:55:08.26756Z","shell.execute_reply":"2025-03-04T00:55:29.887296Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Collecting bitsandbytes\n  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nCollecting xformers==0.0.29\n  Downloading xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nCollecting trl\n  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\nCollecting triton\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading xformers-0.0.29-cp310-cp310-manylinux_2_28_x86_64.whl (15.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, xformers, trl, bitsandbytes\nSuccessfully installed bitsandbytes-0.45.3 triton-3.2.0 trl-0.15.2 xformers-0.0.29\nCollecting cut_cross_entropy\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting unsloth_zoo\n  Downloading unsloth_zoo-2025.2.7-py3-none-any.whl.metadata (16 kB)\nDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading unsloth_zoo-2025.2.7-py3-none-any.whl (107 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/107.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: unsloth_zoo, cut_cross_entropy\nSuccessfully installed cut_cross_entropy-25.1.1 unsloth_zoo-2025.2.7\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.28.1)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.10/dist-packages (0.1.9)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nCollecting unsloth\n  Downloading unsloth-2025.2.15-py3-none-any.whl.metadata (57 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth-2025.2.15-py3-none-any.whl (188 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.8/188.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: unsloth\nSuccessfully installed unsloth-2025.2.15\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch accelerate transformers bitsandbytes peft","metadata":{"_uuid":"f90269c9-ffcf-4fc2-b7ec-c1220d79aaae","_cell_guid":"4d59d6b2-9da4-4a1b-8e80-0762fc233278","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-03-04T00:55:35.257251Z","iopub.execute_input":"2025-03-04T00:55:35.257568Z","iopub.status.idle":"2025-03-04T00:55:38.900704Z","shell.execute_reply.started":"2025-03-04T00:55:35.257542Z","shell.execute_reply":"2025-03-04T00:55:38.899893Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.2.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.45.3)\nRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.28.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install --upgrade  torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T00:55:41.262417Z","iopub.execute_input":"2025-03-04T00:55:41.262709Z","iopub.status.idle":"2025-03-04T00:58:33.301676Z","shell.execute_reply.started":"2025-03-04T00:55:41.262685Z","shell.execute_reply":"2025-03-04T00:58:33.300819Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nCollecting torchvision\n  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nCollecting torch==2.6.0 (from torchvision)\n  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (2024.9.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchvision)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->torchvision)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth 2025.2.15 requires tyro, which is not installed.\nunsloth-zoo 2025.2.7 requires tyro, which is not installed.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 24.12.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 24.12.1 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\nunsloth 2025.2.15 requires transformers!=4.47.0,>=4.46.1, but you have transformers 4.47.0 which is incompatible.\nxformers 0.0.29 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.6.0 torchvision-0.21.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall \"numpy<2.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:52:02.027617Z","iopub.execute_input":"2025-02-26T20:52:02.027933Z","iopub.status.idle":"2025-02-26T20:52:09.469906Z","shell.execute_reply.started":"2025-02-26T20:52:02.027907Z","shell.execute_reply":"2025-02-26T20:52:09.46904Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy<2.0\n  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth 2025.2.15 requires tyro, which is not installed.\nunsloth-zoo 2025.2.7 requires tyro, which is not installed.\ncudf-cu12 24.12.0 requires pyarrow<19.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 19.0.0 which is incompatible.\nlangchain 0.3.12 requires async-timeout<5.0.0,>=4.0.0; python_version < \"3.11\", but you have async-timeout 5.0.1 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\nxformers 0.0.29 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install importlib-metadata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:52:16.132243Z","iopub.execute_input":"2025-02-26T20:52:16.132554Z","iopub.status.idle":"2025-02-26T20:52:19.771359Z","shell.execute_reply.started":"2025-02-26T20:52:16.13253Z","shell.execute_reply":"2025-02-26T20:52:19.770422Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (8.5.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata) (3.21.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Single GPU FSDP1\n\n![image](loss_plot.png)","metadata":{}},{"cell_type":"code","source":"%%writefile single_gpu.py\nimport os\nimport random\nimport numpy as np\nimport torch\n\nseed = 3407\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# --- Environment Setup ---\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    CPUOffload,\n    ShardingStrategy,\n    MixedPrecision,\n)\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\nfrom functools import partial\n\n# --- 1) Distributed Setup ---\nif \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n    dist.init_process_group(backend=\"nccl\")\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\nelse:\n    local_rank = 0\n\naccelerator = Accelerator(mixed_precision=\"bf16\")\ndevice = torch.device(\"cuda\", local_rank)\n\n# --- 2) Model & 4-bit Quantization ---\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\ndtype = torch.bfloat16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n    bnb_4bit_quant_storage=dtype,  # try storing quantized weights as BF16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 3) Apply LoRA ---\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\nfor name, param in model.named_parameters():\n    if \".lora_\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# --- Integrate torch.compile (PyTorch 2.0 feature) ---\ntry:\n    model = torch.compile(model)\n    accelerator.print(\"torch.compile: Model compiled successfully.\")\nexcept Exception as e:\n    accelerator.print(f\"torch.compile: Compilation failed with error: {e}. Continuing without compilation.\")\n\ntorch.cuda.empty_cache()\n\n# --- Debug: Print buffer names and dtypes ---\nif local_rank == 0:\n    buffer_info = [(name, buf.dtype) for name, buf in model.named_buffers()]\n    print(\"Buffers in model (name, dtype):\")\n    for name, dt in buffer_info:\n        print(name, dt)\n\n# --- 5) FSDP Wrapping with FSDP1 ---\n# Ignore the frozen base module from FSDP sharding.\nbase_model_submodule = model.model  \nignored_modules = [base_model_submodule]\n\nauto_wrap_policy = partial(\n    size_based_auto_wrap_policy,\n    min_num_params=5e7,\n    recurse=True,\n    nonwrapped_numel=0,\n)\n\nmodel = FSDP(\n    model,\n    auto_wrap_policy=auto_wrap_policy,\n    sharding_strategy=ShardingStrategy.FULL_SHARD,  # Ensure full sharding (FSDP1)\n    use_orig_params=True,\n    mixed_precision=MixedPrecision(param_dtype=dtype),\n    ignored_modules=ignored_modules,\n    device_id=device,\n)\n\n# --- 6) Tokenizer & Dataset ---\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n\ndataset = load_dataset(\n    \"json\",\n    data_files={\"train\": \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"},\n    split=\"train[:10%]\"\n)\n\ndef tokenize_example(ex):\n    enc = tokenizer(\n        ex[\"text\"],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n    return enc\n\ndataset = dataset.map(tokenize_example, batched=True, remove_columns=[\"text\"])\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntrain_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\ntrain_dataloader = accelerator.prepare(train_dataloader)\n\n# --- 7) Optimizer ---\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\ntry:\n    from bitsandbytes.optim import Adam8bit\n    optimizer = Adam8bit(trainable_params, lr=2e-4)\n    accelerator.print(\"Using bitsandbytes Adam8bit optimizer.\")\nexcept ImportError:\n    optimizer = torch.optim.AdamW(trainable_params, lr=2e-4)\n    accelerator.print(\"Using torch.optim.AdamW optimizer.\")\noptimizer = accelerator.prepare(optimizer)\n\n# --- 8) Training Loop with Gradient Accumulation ---\nloss_history = []\nmodel.train()\ngradient_accumulation_steps = 16  # Adjust this value as needed\n\nfor step, batch in enumerate(train_dataloader, start=1):\n    # The accelerator.accumulate context will accumulate gradients over the specified steps.\n    with accelerator.accumulate(model):\n        outputs = model(**batch)\n        # Scale loss to account for gradient accumulation\n        loss = outputs.loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n        # Clear cache after the optimizer step to lower VRAM usage\n        if accelerator.sync_gradients:\n            torch.cuda.empty_cache()\n    \n    if accelerator.is_main_process:\n        loss_history.append((step, outputs.loss.item()))\n        accelerator.print(f\"Step {step} - Loss: {outputs.loss.item():.4f}\")\n    \n    # Terminate after 60 steps (micro-batches)\n    if step >= 60:\n        break\naccelerator.wait_for_everyone()\n\nif accelerator.is_main_process:\n    try:\n        import matplotlib.pyplot as plt\n        steps, losses = zip(*loss_history) if loss_history else ([], [])\n        if steps:\n            plt.figure(figsize=(8, 4))\n            plt.plot(steps, losses, marker=\"o\")\n            plt.xlabel(\"Training Step\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss over the First 60 Training Steps\")\n            plt.grid(True)\n            plt.savefig(\"loss_plot.png\")\n            plt.show()\n        else:\n            print(\"No loss data recorded.\")\n    except ImportError:\n        accelerator.print(\"matplotlib is not installed. Skipping loss plot.\")\n\n# --- 9) Save Model ---\nif accelerator.is_main_process:\n    final_model = accelerator.unwrap_model(model)\n    final_model.save_pretrained(\"llama-8b-finetuned\", safe_serialization=True)\n    tokenizer.save_pretrained(\"llama-8b-finetuned\")\n\nif dist.is_initialized():\n    dist.destroy_process_group()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!torchrun --nproc_per_node=2 --nnodes=1 single_gpu.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FSDP2 without Gradient Accumulation\n![image](loss_plot_v2.png)","metadata":{}},{"cell_type":"code","source":"%%writefile finetune_llama31_8b_fsdp2_qlora_revised_v2.py\nimport os\n\nimport random\nimport numpy as np\nimport torch\n\nseed = 3407\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Optionally, enforce determinism (may slow down training):\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False\n\n\n# --- Environment Setup ---\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import (\n    FullyShardedDataParallel as FSDP,\n    CPUOffload,\n    ShardingStrategy,\n    MixedPrecision,\n)\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\nfrom functools import partial\n\n# --- 1) Distributed Setup ---\nif \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n    dist.init_process_group(backend=\"nccl\")\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\nelse:\n    local_rank = 0\n\naccelerator = Accelerator(mixed_precision=\"bf16\")\ndevice = torch.device(\"cuda\", local_rank)\n\n# --- 2) Model & 4-bit Quantization ---\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\ndtype = torch.bfloat16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n    bnb_4bit_quant_storage=dtype,  # try storing quantized weights as BF16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 3) Apply LoRA ---\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\nfor name, param in model.named_parameters():\n    if \".lora_\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# --- Integrate torch.compile (PyTorch 2.0 feature) ---\ntry:\n    model = torch.compile(model)\n    accelerator.print(\"torch.compile: Model compiled successfully.\")\nexcept Exception as e:\n    accelerator.print(f\"torch.compile: Compilation failed with error: {e}. Continuing without compilation.\")\n\n# --- Optionally, we previously converted int parameters to buffers.\n# For this version, we skip that conversion and rely on bitsandbytes to manage quantized weights.\n# def convert_int_params_to_buffers(module: torch.nn.Module):\n#     for name, param in list(module.named_parameters(recurse=False)):\n#         if param is not None and not param.dtype.is_floating_point:\n#             del module._parameters[name]\n#             module.register_buffer(name, param.data)\n#     for child in module.children():\n#         convert_int_params_to_buffers(child)\n#\n# convert_int_params_to_buffers(model)\n\ntorch.cuda.empty_cache()\n\n# --- Debug: Print buffer names and dtypes ---\nif local_rank == 0:\n    buffer_info = [(name, buf.dtype) for name, buf in model.named_buffers()]\n    print(\"Buffers in model (name, dtype):\")\n    for name, dt in buffer_info:\n        print(name, dt)\n\n# --- 5) FSDP Wrapping ---\n# Ignore the frozen base module from FSDP sharding.\nbase_model_submodule = model.model  \nignored_modules = [base_model_submodule]\n\nauto_wrap_policy = partial(\n    size_based_auto_wrap_policy,\n    min_num_params=5e7,\n    recurse=True,\n    nonwrapped_numel=0,\n)\n\nmp_policy = MixedPrecision(\n    param_dtype=dtype,      # cast parameters to BF16\n    reduce_dtype=dtype,     # use BF16 for gradient reduction\n    buffer_dtype=None,      # do not cast buffers\n)\n\ncpu_offload = None\n# Uncomment the next line if needed:\n# cpu_offload = CPUOffload(offload_params=True)\n\nmodel = FSDP(\n    model,\n    auto_wrap_policy=auto_wrap_policy,\n    sharding_strategy=ShardingStrategy.FULL_SHARD,\n    use_orig_params=True,\n    cpu_offload=cpu_offload,\n    limit_all_gathers=True,\n    device_id=device,\n    ignored_modules=ignored_modules,\n    mixed_precision=mp_policy,\n)\n\n# --- 6) Tokenizer & Dataset ---\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n\ndataset = load_dataset(\n    \"json\",\n    data_files={\"train\": \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"},\n    split=\"train[:10%]\"\n)\n\ndef tokenize_example(ex):\n    enc = tokenizer(\n        ex[\"text\"],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n    return enc\n\ndataset = dataset.map(tokenize_example, batched=True, remove_columns=[\"text\"])\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntrain_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\ntrain_dataloader = accelerator.prepare(train_dataloader)\n\n# --- 7) Optimizer ---\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\ntry:\n    from bitsandbytes.optim import Adam8bit\n    optimizer = Adam8bit(trainable_params, lr=2e-4)\n    accelerator.print(\"Using bitsandbytes Adam8bit optimizer.\")\nexcept ImportError:\n    optimizer = torch.optim.AdamW(trainable_params, lr=2e-4)\n    accelerator.print(\"Using torch.optim.AdamW optimizer.\")\noptimizer = accelerator.prepare(optimizer)\n\n# --- 8) Training Loop ---\nloss_history = []\nmodel.train()\nfor step, batch in enumerate(train_dataloader, start=1):\n    outputs = model(**batch)\n    loss = outputs.loss\n    accelerator.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n    torch.cuda.empty_cache()\n    if accelerator.is_main_process:\n        loss_history.append((step, loss.item()))\n    \n    accelerator.print(f\"Step {step} - Loss: {loss.item():.4f}\")\n    \n    # Terminate after 60 steps.\n    if step >= 60:\n        break\naccelerator.wait_for_everyone()\n\nif accelerator.is_main_process:\n    try:\n        import matplotlib.pyplot as plt\n        steps, losses = zip(*loss_history) if loss_history else ([], [])\n        if steps:\n            plt.figure(figsize=(8, 4))\n            plt.plot(steps, losses, marker=\"o\")\n            plt.xlabel(\"Training Step\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss over the First 60 Training Steps\")\n            plt.grid(True)\n            plt.savefig(\"loss_plot_v2.png\")\n            plt.show()\n        else:\n            print(\"No loss data recorded.\")\n    except ImportError:\n        accelerator.print(\"matplotlib is not installed. Skipping loss plot.\")\n\n\n# --- 9) Save Model ---\nif accelerator.is_main_process:\n    final_model = accelerator.unwrap_model(model)\n    final_model.save_pretrained(\"llama-8b-finetuned\", safe_serialization=True)\n    tokenizer.save_pretrained(\"llama-8b-finetuned\")\n\nif dist.is_initialized():\n    dist.destroy_process_group()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch --num_processes=2 --mixed_precision=bf16 finetune_llama31_8b_fsdp2_qlora_revised_v2.py","metadata":{"_uuid":"ca3d9d5c-55c6-40ee-a924-0339e2cf6a81","_cell_guid":"d9fc9790-f801-4b76-af39-38bd4a1c11ba","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FSDP2 with Gradient Accumulation\n![image](loss_plot_gradient_accum.png)","metadata":{}},{"cell_type":"code","source":"%%writefile gradient_accumulation.py\nimport os\nimport random\nimport numpy as np\nimport torch\n\nseed = 3407\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Optionally, enforce determinism (may slow down training):\n# torch.backends.cudnn.deterministic = True\n# torch.backends.cudnn.benchmark = False\n\n# --- Environment Setup ---\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nimport torch.distributed as dist\n# Import FSDP2 from the new location.\nfrom torch.distributed.fsdp import fully_shard as FSDP2\nfrom torch.distributed.fsdp import CPUOffload, ShardingStrategy, MixedPrecision\nfrom torch.distributed.fsdp.wrap import size_based_auto_wrap_policy\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import load_dataset\nfrom accelerate import Accelerator\nfrom functools import partial\n\n# --- 1) Distributed Setup ---\nif \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n    dist.init_process_group(backend=\"nccl\")\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\nelse:\n    local_rank = 0\n\naccelerator = Accelerator(mixed_precision=\"bf16\")\ndevice = torch.device(\"cuda\", local_rank)\n\n# --- 2) Model & 4-bit Quantization ---\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\ndtype = torch.bfloat16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n    bnb_4bit_quant_storage=dtype,  # store quantized weights as BF16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 3) Apply LoRA ---\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\n# Freeze all parameters except LoRA ones.\nfor name, param in model.named_parameters():\n    if \".lora_\" in name:\n        param.requires_grad = True\n    else:\n        param.requires_grad = False\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\n\n# --- Integrate torch.compile (PyTorch 2.0 feature) ---\ntry:\n    model = torch.compile(model)\n    accelerator.print(\"torch.compile: Model compiled successfully.\")\nexcept Exception as e:\n    accelerator.print(f\"torch.compile: Compilation failed with error: {e}. Continuing without compilation.\")\n\ntorch.cuda.empty_cache()\n\n# --- Debug: Print buffer names and dtypes ---\nif local_rank == 0:\n    buffer_info = [(name, buf.dtype) for name, buf in model.named_buffers()]\n    print(\"Buffers in model (name, dtype):\")\n    for name, dt in buffer_info:\n        print(name, dt)\n\n# --- 5) FSDP2 Wrapping ---\n# Ignore the frozen base module from FSDP sharding.\nbase_model_submodule = model.model  \nignored_modules = [base_model_submodule]\n\nauto_wrap_policy = partial(\n    size_based_auto_wrap_policy,\n    min_num_params=5e7,\n    recurse=True,\n    nonwrapped_numel=0,\n)\n\nmp_policy = MixedPrecision(\n    param_dtype=dtype,      # cast parameters to BF16\n    reduce_dtype=dtype,     # use BF16 for gradient reduction\n    buffer_dtype=None,      # do not cast buffers\n)\n\n# Uncomment the next line if you want CPU offloading.\ncpu_offload = CPUOffload(offload_params=True)\n# Alternatively, set cpu_offload = None if offloading is not desired.\n\nmodel = FSDP2(\n    model,\n    auto_wrap_policy=auto_wrap_policy,\n    sharding_strategy=ShardingStrategy.FULL_SHARD,\n    use_orig_params=True,\n    cpu_offload=cpu_offload,\n    limit_all_gathers=True,\n    device_id=device,\n    ignored_modules=ignored_modules,\n    mixed_precision=mp_policy,\n)\n\n# --- 6) Tokenizer & Dataset ---\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.padding_side = \"right\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n\ndataset = load_dataset(\n    \"json\",\n    data_files={\"train\": \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"},\n    split=\"train[:10%]\"\n)\n\ndef tokenize_example(ex):\n    enc = tokenizer(\n        ex[\"text\"],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True,\n    )\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n    return enc\n\ndataset = dataset.map(tokenize_example, batched=True, remove_columns=[\"text\"])\ndataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntrain_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\ntrain_dataloader = accelerator.prepare(train_dataloader)\n\n# --- 7) Optimizer ---\ntrainable_params = [p for p in model.parameters() if p.requires_grad]\ntry:\n    from bitsandbytes.optim import Adam8bit\n    optimizer = Adam8bit(trainable_params, lr=2e-4)\n    accelerator.print(\"Using bitsandbytes Adam8bit optimizer.\")\nexcept ImportError:\n    optimizer = torch.optim.AdamW(trainable_params, lr=2e-4)\n    accelerator.print(\"Using torch.optim.AdamW optimizer.\")\noptimizer = accelerator.prepare(optimizer)\n\n# --- 8) Training Loop with Gradient Accumulation ---\nloss_history = []\nmodel.train()\ngradient_accumulation_steps = 16  # Adjust this value as needed\n\nfor step, batch in enumerate(train_dataloader, start=1):\n    # The accelerator.accumulate context will accumulate gradients over the specified steps.\n    with accelerator.accumulate(model):\n        outputs = model(**batch)\n        # Scale loss to account for gradient accumulation\n        loss = outputs.loss / gradient_accumulation_steps\n        accelerator.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n        # Clear cache after the optimizer step to lower VRAM usage\n        if accelerator.sync_gradients:\n            torch.cuda.empty_cache()\n    \n    if accelerator.is_main_process:\n        loss_history.append((step, outputs.loss.item()))\n        accelerator.print(f\"Step {step} - Loss: {outputs.loss.item():.4f}\")\n    \n    # Terminate after 60 steps (micro-batches)\n    if step >= 60:\n        break\naccelerator.wait_for_everyone()\n\nif accelerator.is_main_process:\n    try:\n        import matplotlib.pyplot as plt\n        steps, losses = zip(*loss_history) if loss_history else ([], [])\n        if steps:\n            plt.figure(figsize=(8, 4))\n            plt.plot(steps, losses, marker=\"o\")\n            plt.xlabel(\"Training Step\")\n            plt.ylabel(\"Loss\")\n            plt.title(\"Loss over the First 60 Training Steps\")\n            plt.grid(True)\n            plt.savefig(\"loss_plot_gradient_accum.png\")\n            plt.show()\n        else:\n            print(\"No loss data recorded.\")\n    except ImportError:\n        accelerator.print(\"matplotlib is not installed. Skipping loss plot.\")\n\n# --- 9) Save Model ---\nif accelerator.is_main_process:\n    final_model = accelerator.unwrap_model(model)\n    final_model.save_pretrained(\"llama-8b-finetuned\", safe_serialization=True)\n    tokenizer.save_pretrained(\"llama-8b-finetuned\")\n\nif dist.is_initialized():\n    dist.destroy_process_group()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:55:53.899311Z","iopub.execute_input":"2025-02-26T20:55:53.899615Z","iopub.status.idle":"2025-02-26T20:55:53.906269Z","shell.execute_reply.started":"2025-02-26T20:55:53.899591Z","shell.execute_reply":"2025-02-26T20:55:53.9055Z"}},"outputs":[{"name":"stdout","text":"Overwriting gradient_accumulation.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile gradient_accumulation.py\nimport os\nimport random\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import fully_shard, MixedPrecision, OffloadPolicy\nfrom torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (\n    apply_activation_checkpointing,\n    checkpoint_wrapper,\n)\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    BitsAndBytesConfig,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers.models.llama.modeling_llama import LlamaDecoderLayer\nfrom datasets import load_dataset\n\n# --- 1) Environment Setup & Seeds ---\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128,expandable_segments:True\"\nos.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n\nseed = 3407\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# --- 2) Distributed Setup ---\ndist.init_process_group(backend=\"nccl\")\nlocal_rank = int(os.environ[\"LOCAL_RANK\"])\nworld_size = int(os.environ[\"WORLD_SIZE\"])\ntorch.cuda.set_device(local_rank)\n\n# --- 3) Model & Tokenizer & Quantization ---\nmodel_name = \"unsloth/meta-Llama-3.1-8B-Instruct-bnb-4bit\"\ndtype = torch.bfloat16\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=dtype,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif not tokenizer.pad_token_id:\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n    device_map=None,\n)\nmodel = prepare_model_for_kbit_training(model)\n\n# --- 4) LoRA Configuration ---\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=128,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_config)\n\n# Freeze non-LoRA parameters and ensure valid dtypes\nfor name, param in model.named_parameters():\n    if \".lora_\" not in name:\n        param.requires_grad = False\n        \n        # Ensure the parameter is a floating-point type\n        if param.dtype != torch.float32 and param.dtype != torch.float16 and param.dtype != torch.bfloat16:\n            print(f\"Converting parameter {name} from {param.dtype} to bfloat16.\")\n            param.data = param.data.to(torch.bfloat16)\n\n# --- 5) Dataset Preparation ---\ndef prepare_dataset():\n    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n\n    def tokenize_function(examples):\n        return tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding=\"max_length\",\n            max_length=512,\n        )\n\n    tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n    return tokenized_dataset\n\n# --- 6) FSDP2 Configuration ---\nmp_policy = MixedPrecision(\n    param_dtype=torch.bfloat16,\n    reduce_dtype=torch.bfloat16,\n    buffer_dtype=torch.bfloat16,\n)\n\ndef check_fn(submodule):\n    return isinstance(submodule, (LlamaDecoderLayer,))\n\napply_activation_checkpointing(\n    model,\n    checkpoint_wrapper_fn=checkpoint_wrapper,\n    check_fn=check_fn,\n)\n\nmodel = fully_shard(\n    model,\n    mp_policy=mp_policy,\n    offload_policy=OffloadPolicy(),\n    reshard_after_forward=True,\n)\n\n# --- 7) HuggingFace Trainer Setup ---\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=3,\n    per_device_train_batch_size=1,  # Consider reducing this if OOM persists\n    gradient_accumulation_steps=16,\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=True,  # Using bfloat16 precision\n    logging_steps=10,\n    save_steps=100,\n    evaluation_strategy=\"steps\",\n    eval_steps=100,\n    save_total_limit=2,\n)\n\n# --- 8) Trainer Setup ---\nfrom transformers import DataCollatorForLanguageModeling\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=prepare_dataset(),\n    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n)\n\n# --- 9) Training ---\ntry:\n    trainer.train()\nexcept RuntimeError as e:\n    if 'out of memory' in str(e):\n        print(\"Caught OOM error! Consider reducing batch size or using CPU offloading.\")\n        raise e\n\n# --- 10) Model Saving ---\nif local_rank == 0:\n    trainer.save_model(\"llama-8b-finetuned\")\n    tokenizer.save_pretrained(\"llama-8b-finetuned\")\n\ndist.destroy_process_group()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T01:27:15.725383Z","iopub.execute_input":"2025-03-04T01:27:15.725712Z","iopub.status.idle":"2025-03-04T01:27:15.731742Z","shell.execute_reply.started":"2025-03-04T01:27:15.725688Z","shell.execute_reply":"2025-03-04T01:27:15.730893Z"}},"outputs":[{"name":"stdout","text":"Overwriting gradient_accumulation.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!torchrun --standalone --nnodes=1 --nproc_per_node=2 gradient_accumulation.py\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T01:27:19.937839Z","iopub.execute_input":"2025-03-04T01:27:19.938176Z","iopub.status.idle":"2025-03-04T01:28:30.228167Z","shell.execute_reply.started":"2025-03-04T01:27:19.938148Z","shell.execute_reply":"2025-03-04T01:28:30.226878Z"}},"outputs":[{"name":"stdout","text":"W0304 01:27:21.819000 224 torch/distributed/run.py:792] \nW0304 01:27:21.819000 224 torch/distributed/run.py:792] *****************************************\nW0304 01:27:21.819000 224 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0304 01:27:21.819000 224 torch/distributed/run.py:792] *****************************************\n[W304 01:27:32.632184263 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W304 01:27:42.642664837 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n2025-03-04 01:27:46.831552: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-04 01:27:46.841127: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-04 01:27:46.853322: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-04 01:27:46.860483: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-03-04 01:27:46.862712: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-04 01:27:46.869024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[W304 01:28:00.730052842 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W304 01:28:00.805746857 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W304 01:28:10.736648298 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n[W304 01:28:20.747064921 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nUnused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nConverting parameter base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.0.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.1.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.2.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.3.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.4.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.5.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.6.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.7.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.8.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.9.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.10.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.11.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.12.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.13.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.14.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.15.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.16.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.17.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.18.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.19.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.20.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.21.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.22.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.23.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.24.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.25.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.26.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.27.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.28.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.29.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.30.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.mlp.gate_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.mlp.up_proj.base_layer.weight from torch.uint8 to bfloat16.\nConverting parameter base_model.model.model.layers.31.mlp.down_proj.base_layer.weight from torch.uint8 to bfloat16.\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/working/gradient_accumulation.py\", line 114, in <module>\n[rank0]:     model = fully_shard(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/_composable/contract.py\", line 125, in wrapper\n[rank0]:     updated = func(inp_module, *args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fully_shard.py\", line 176, in fully_shard\n[rank0]:     state._fsdp_param_group = FSDPParamGroup(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 131, in __init__\n[rank0]:     self.fsdp_params = [\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 132, in <listcomp>\n[rank0]:     FSDPParam(\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py\", line 239, in __init__\n[rank0]:     self._init_sharded_param(param, device, shard_placement_fn)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[rank0]:     return func(*args, **kwargs)\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py\", line 377, in _init_sharded_param\n[rank0]:     padded_sharded_param = param_data.new_zeros(padded_sharded_size)\n[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 21697 has 14.73 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 253.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/kaggle/working/gradient_accumulation.py\", line 114, in <module>\n[rank1]:     model = fully_shard(\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/_composable/contract.py\", line 125, in wrapper\n[rank1]:     updated = func(inp_module, *args, **kwargs)\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fully_shard.py\", line 176, in fully_shard\n[rank1]:     state._fsdp_param_group = FSDPParamGroup(\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 131, in __init__\n[rank1]:     self.fsdp_params = [\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param_group.py\", line 132, in <listcomp>\n[rank1]:     FSDPParam(\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py\", line 239, in __init__\n[rank1]:     self._init_sharded_param(param, device, shard_placement_fn)\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n[rank1]:     return func(*args, **kwargs)\n[rank1]:   File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/fsdp/_fully_shard/_fsdp_param.py\", line 377, in _init_sharded_param\n[rank1]:     padded_sharded_param = param_data.new_zeros(padded_sharded_size)\n[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.00 MiB. GPU 1 has a total capacity of 14.74 GiB of which 6.12 MiB is free. Process 21698 has 14.73 GiB memory in use. Of the allocated memory 14.37 GiB is allocated by PyTorch, and 253.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n[rank0]:[W304 01:28:29.535301257 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nW0304 01:28:29.641000 224 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 229 closing signal SIGTERM\nE0304 01:28:29.705000 224 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 228) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"/usr/local/bin/torchrun\", line 8, in <module>\n    sys.exit(main())\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 918, in main\n    run(args)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 909, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n============================================================\ngradient_accumulation.py FAILED\n------------------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n------------------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-03-04_01:28:29\n  host      : b78741b7afa8\n  rank      : 0 (local_rank: 0)\n  exitcode  : 1 (pid: 228)\n  error_file: <N/A>\n  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n============================================================\n","output_type":"stream"}],"execution_count":13}]}